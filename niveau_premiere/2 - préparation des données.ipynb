{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7277555-182a-49fe-9c41-453a6f4c9fef",
   "metadata": {},
   "source": [
    "# Génération de musique par Intelligence Artificielle pour la pratique sportive \r\n",
    "\r\n",
    "Nous nous proposons d'ajuster/affiner (*\"fine-tune\"*) l'algorithme MusicGen afin de répondre au mieux au besoin d'utilisateurs de l'application [Decathlon Coach](https://www.decathloncoach.com/fr/home/coaching/programs/sport) désireux de générer des musiques pour leurs séances sportives. Il s'agit d'un cas d'usage fictif mais qui nous permettra de manipuler MusicGen. Nous supposons que les utilisateurs en question souhaitent écouter des musiques qui sont à la fois en accord avec leurs goûts musicaux, mais aussi adaptés à chaque séance (exemples: musiques plus calmes pour du Yoga, plus toniques pour du cardio, etc...).\r\n",
    "\r\n",
    "Avant d'apporter des modifications à MusicGen, il est indispensable de préparer les données que l'on souhaite utilisée (collecte, formatage, etc...). C'est l'objet du présent noteb\n",
    "\n",
    "## Partie 2: préparation des données\n",
    "\n",
    "Les 2 cellules ci-dessous chargent quelques modules utilitaires que nous ne détaillerons pas ici. pas ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cec7e1-f235-4e38-ab62-83f1a4634288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run utilitaires.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a219e6f-6a0e-4a83-aaec-14e257b4a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e81da-c4f7-4283-bbed-11e4bffe8922",
   "metadata": {},
   "source": [
    "### Collecte et analyse de la donnée brute\r\n",
    "\r\n",
    "Nous prendrons l'exemple d'un utilisateur adepte de musique latine pour lequel on a collecté un échantillon de plusieurs musiques qu'il a déjà écouté durant ses séances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd440699-6447-4279-9dc4-c115e7085fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_data = [\n",
    "    \"./data/music/raw/latino\",\n",
    "    \"./data/music/raw/classic\",\n",
    "    # \"./data/music/raw/electro\",\n",
    "]\n",
    "music_files = [\n",
    "    os.sep.join([path_to_data, f]) \n",
    "    for path_to_data in paths_to_data \n",
    "    for f in os.listdir(path_to_data) \n",
    "    if f[-4:] == \".mp3\"\n",
    "]\n",
    "\n",
    "for f in music_files[:3]:\n",
    "    print(\" \".join(f[:-4].split(os.sep)[-1].split(\"_\") + [\":\"]))\n",
    "    display(Audio(f, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f430bbe-9706-4f7f-8f1b-8d40ef1b6c0d",
   "metadata": {},
   "source": [
    "\n",
    "### Découpage des fichiers musicaux en \"chunks\"\r\n",
    "\r\n",
    "Nous devons tout d'abord \"découper\" les morceaux de musiques en \"portions\" (*\"chunks\"*) plus court, et pour des questions d'efficacité computationelles, de e longuesur égales.\r\n",
    "\r\n",
    "Chaque portion, aura une durée de 25 secondes, et nous autorisons un chevauchement de 10 secondes entre chaque morceau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa9cb5-b17c-4418-a8db-31ec413e53de",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder_name = \"facebook/encodec_32khz\"\n",
    "\n",
    "audio_encoder_feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    audio_encoder_name,\n",
    "    cache_dir=\"./models\"\n",
    ")\n",
    "\n",
    "chunk_duration = 25\n",
    "chunk_overlap = 10\n",
    "sampling_rate = audio_encoder_feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c8659-5052-43f9-9cce-d8093898b712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunks = chunk_audio_file(music_files[0], chunk_duration, chunk_overlap, sampling_rate=sampling_rate)\n",
    "np.testing.assert_allclose(\n",
    "    chunks[0][\"array\"][-chunks[0][\"sampling_rate\"] * chunk_overlap:],\n",
    "    chunks[1][\"array\"][:chunks[0][\"sampling_rate\"] * chunk_overlap]\n",
    ")\n",
    "for chunk in chunks:\n",
    "    display(Audio(chunk[\"array\"], rate=chunk[\"sampling_rate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f7a31-53ee-4979-87af-552eddaedfd7",
   "metadata": {},
   "source": [
    "Nous utilisons la fonction `prompt_engineering` introduite dans le notebook précédent pour \"étiqueter\" chaque morceau avec un texte qui le décrit.\n",
    "\n",
    "Nous obtenons un jeux de données (*\"dataset\"*) contenant 2 colonnes (et une cinquantaine de lignes):\n",
    "- `audio` (morceaux de musiques de 25 secondes chacun)\n",
    "- `description` (description textuelle générée automatiquement en spécifiant uniquement le nom de la session sportive et le style de musique souhaité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc493881-0e8d-4afe-bc1d-c3a0fa2854a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list([\n",
    "    {\n",
    "        \"audio\": chunk,\n",
    "        \"description\": prompt_engineering(\n",
    "            ' '.join(f[:-4].split(os.sep)[-1].split('_')[:-1]), \n",
    "            f[:-4].split(os.sep)[-1].split('_')[-1]\n",
    "        ),\n",
    "    } for chunk in chunk_audio_file(f, chunk_duration, chunk_overlap, sampling_rate=sampling_rate) for f in music_files\n",
    "    if len(chunk[\"array\"]) == sampling_rate * chunk_duration\n",
    "])\n",
    "print(dataset)\n",
    "all_descriptions = {\n",
    "    k: len(list(g)) \n",
    "    for k, g in itertools.groupby(sorted(dataset[\"description\"]))\n",
    "}\n",
    "pprint(all_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b03cd5-256f-49f4-9992-6c4fd40000df",
   "metadata": {},
   "source": [
    "### \"Tokenization\" des descriptions textuelles\r\n",
    "\r\n",
    "Comme tous les modèles d'IA, MusicGen ne comprend pas le langage \"naturel\", il ne sait manipuler que des nombres.\r\n",
    "\r\n",
    "Nous devons donc transformer le texte en nombres \"compréhensibles\" pour MusicGen. Ces nombre sont également appelés \"jetons\" (*\"tokens\"*).\r\n",
    "\r\n",
    "L'algorithme opérant cette transformation est appelé *\"tokenizer\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa795c-0302-485a-8642-8ebc679719e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoProcessor.from_pretrained(\n",
    "    \"facebook/musicgen-medium\",\n",
    "    cache_dir=\"./models\"\n",
    ").tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1597c5-f501-49ba-96f0-b849ea52979b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint({\n",
    "    k: tokenizer(k)[\"input_ids\"]\n",
    "     for k in all_descriptions.keys()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5aeeb1-a4d7-4473-b1ff-f09e5d14a707",
   "metadata": {},
   "source": [
    "On observe que le premier token est toujours 3, et le dernier toujours 1.\n",
    "Essayons de voir à quoi ces tokens correspondent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c7a11-2cc5-4e51-bd06-be5c8cc4fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(1))\n",
    "print(tokenizer.decode(14098))\n",
    "print(tokenizer.decode(32))\n",
    "tokenizer.decode(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9873f-54f6-48ee-a51c-22ba28c7ed32",
   "metadata": {},
   "source": [
    "Pour des raisons d'efficacité computationnelle, toutes les entrées doivent avoir la même \"taille\" (même nombrer de tokens). \n",
    "\n",
    "Pour se faire, on se définit une longueur de référence (`max_length = 20` tokens ci-dessous). On \"tronque\" (*\"truncation\"*) les textes trop longs, et on \"complète\" (*\"padding\"*) les textes trop courts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff5b63-598a-4f79-b581-8409f3c81345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, batch, text_column, max_length):\n",
    "    return tokenizer(\n",
    "        batch[text_column],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb45e5-8f28-488b-85ab-e41f8cac4b84",
   "metadata": {},
   "source": [
    "On applique la *\"tokenization\"* sur l'intégralité de la colonne `description` du jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831e6f3-b96f-4c48-86ff-34e9620d9232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda batch: tokenize(tokenizer, batch, text_column=\"description\", max_length=20), \n",
    "    num_proc=18,\n",
    "    desc=\"Tokenize descriptions\",\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ed5f6-a1b3-4348-a96d-957c9d99ed09",
   "metadata": {},
   "source": [
    "### Formatage (encodage) de l'audio\n",
    "\n",
    "De même que pour le texte, pour l'audio MusicGen attend un format bien spécifique.\n",
    "\n",
    "Dans un premier temps nous extrayons l'audio sous forme d'une liste de valeurs numériques dans une nouvelle colonne du jeux de données que l'on baptise `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93acd9-822f-4f3b-88fa-ff40568758c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_encoder_feature_extractor, batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    labels = audio_encoder_feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "    )\n",
    "    batch[\"labels\"] = labels[\"input_values\"]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda batch: extract_audio_features(audio_encoder_feature_extractor, batch), \n",
    "    num_proc=18,\n",
    "    desc=\"Extract audio features\",\n",
    ")\n",
    "print(dataset)\n",
    "print(np.asarray(dataset[0][\"labels\"]).shape)\n",
    "print(dataset[25][\"labels\"][0][0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f34dc6-1bcc-4a86-8f66-7a53291007a2",
   "metadata": {},
   "source": [
    "Quelques statistiques pour mieux comprendre la nouvelle colonne `labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461e330-3e4f-4b6d-a530-07f85c47abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = query(f\"\"\"\n",
    "SELECT \n",
    "  max(length(description)) as max_nb_characters,\n",
    "  min(length(description)) as min_nb_characters,\n",
    "  max(length(input_ids)) as max_nb_tokens,\n",
    "  min(length(input_ids)) as min_nb_tokens,\n",
    "  max(length(labels[1][1])) as max_length_labels,\n",
    "  min(length(labels[1][1])) as min_length_labels,\n",
    "FROM dataset\n",
    "\"\"\", load_from_cache_file=False)\n",
    "pprint(stats[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ce153-ded2-4d9b-9027-7a4f5db1b83c",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, nous allons \"encoder\" la liste de valeurs numériques contenues dans `labels` sous la forme d'une suite de \"vecteurs\" de dimension 4. C'est ce format qu'attend MusicGen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662990f-3a4c-4af4-9d35-918fc4936869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTextToWaveform.from_pretrained(\n",
    "    \"facebook/musicgen-medium\", \n",
    "    cache_dir=\"./models\"\n",
    ")\n",
    "audio_encoder = model.audio_encoder\n",
    "num_codebooks = model.decoder.config.num_codebooks\n",
    "audio_encoder_pad_token_id = model.config.decoder.pad_token_id\n",
    "\n",
    "pad_labels = torch.ones((1, 1, num_codebooks, 1)) * audio_encoder_pad_token_id\n",
    "\n",
    "if torch.cuda.device_count() == 1:\n",
    "    audio_encoder.to(\"cuda\")\n",
    "\n",
    "def apply_audio_encoderr(batch):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        labels = audio_encoder.encode(\n",
    "            torch.tensor(batch[\"labels\"]).to(audio_encoder.device)\n",
    "        )[\"audio_codes\"]\n",
    "\n",
    "    # add pad token column\n",
    "    labels = torch.cat(\n",
    "        [pad_labels.to(labels.device).to(labels.dtype), labels], dim=-1\n",
    "    )\n",
    "\n",
    "    labels, delay_pattern_mask = model.decoder.build_delay_pattern_mask(\n",
    "        labels.squeeze(0),\n",
    "        audio_encoder_pad_token_id,\n",
    "        labels.shape[-1] + num_codebooks,\n",
    "    )\n",
    "\n",
    "    labels = model.decoder.apply_delay_pattern_mask(labels, delay_pattern_mask)\n",
    "\n",
    "    # the first timestamp is associated to a row full of BOS, let's get rid of it\n",
    "    batch[\"labels\"] = labels[:, 1:].cpu()\n",
    "    return batch\n",
    "\n",
    "# Encodec doesn't truely support batching\n",
    "# Pass samples one by one to the GPU\n",
    "dataset = dataset.map(\n",
    "    apply_audio_encoderr,\n",
    "    num_proc=1,\n",
    "    desc=\"Apply encodec\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e303c3-65e9-4574-96ee-d099f46becf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(type(dataset[0][\"input_ids\"]))\n",
    "print(type(dataset[0][\"labels\"]))\n",
    "print(np.asarray(dataset[0][\"input_ids\"]).shape)\n",
    "print(np.asarray(dataset[0][\"labels\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2da6fa-52c3-4c88-a5e9-b3dea56be565",
   "metadata": {},
   "source": [
    "### Sauvegarde du jeux de données sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320f739-7086-490c-99d0-bce74de82f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./data/music/processed/all\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "shutil.rmtree(save_path)\n",
    "\n",
    "dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc42c71-7157-4861-9dc6-2b29f2febb8a",
   "metadata": {},
   "source": [
    "Vérifions que la lecture du jeux de donnée s'effectue correctement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ce1f3-0dc9-4ecb-83f0-c2e960734857",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_from_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf2765-793f-457d-9536-2ef1090aa1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
