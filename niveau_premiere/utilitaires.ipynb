{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2552878a-1fd9-4c99-aa59-94bc77a46faf",
   "metadata": {},
   "source": [
    "### Installation des dÃ©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acdb5a-b757-45c9-b1fb-cfbdef5d6bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.48.2 peft==0.14.0 datasets==3.2.0 datasets_sql nbformat librosa soundfile  # ipython==8.32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639459bf-ef09-4b39-bbc6-65a247ccb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import functools\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForTextToWaveform,\n",
    "    AutoFeatureExtractor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    Audio as HFAudio,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from datasets_sql import query\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace575f-dc5f-4c1b-9d8f-bc7543b26090",
   "metadata": {},
   "source": [
    "### Empty GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76765489-baec-46cf-bc61-15f8af532947",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fc8c9-29a8-48c7-ae4d-9cbcdc7847c3",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires additionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39591c9c-ff04-4773-8004-4d8b1fbde9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_music(\n",
    "    processor, \n",
    "    model, \n",
    "    prompts, \n",
    "    max_new_tokens, \n",
    "    sampling_rate, \n",
    "    do_sample, \n",
    "    guidance_scale,\n",
    "):\n",
    "    \"\"\"Converst prompts (list of sting) to list of audio files (musics)\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    audio_values = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=do_sample, \n",
    "        guidance_scale=guidance_scale, \n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    musics = [\n",
    "        Audio(m.squeeze(), rate=sampling_rate)\n",
    "        for m in audio_values.cpu().numpy()\n",
    "    ]\n",
    "    for m in musics:\n",
    "        display(m)\n",
    "    return musics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f494c2-7579-44f7-8bae-0862a290c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_audio_file(audio_file, chunk_duration, chunk_overlap, sampling_rate=None):\n",
    "    \"\"\"Chunks audio file into possibly overlapping (chunk_overlap) shorter pieces (chunk_duration)\"\"\"\n",
    "    y, sr = librosa.load(audio_file, sr=sampling_rate)\n",
    "    chunk_samples = int(chunk_duration * sr)\n",
    "    chunk_overlap *= sr\n",
    "    chunks = [\n",
    "        {\n",
    "            \"array\": y[i:i + chunk_samples],\n",
    "            \"sampling_rate\": sr,\n",
    "        }\n",
    "        for i in range(0, len(y), chunk_samples - chunk_overlap)\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50108166-a48f-4232-ab56-483b3a6a36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_engineering(sport_session_name: str, music_style: str) -> str:\n",
    "    return f\"{music_style} music for {sport_session_name} sport session\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910814c0-d699-4a2c-889a-996774d4762f",
   "metadata": {},
   "source": [
    "## Classes utilitaires additionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd0623-02e0-43d7-aa00-88a572e12351",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorMusicGenWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.AutoProcessor`)\n",
    "            The processor used for proccessing the data.\n",
    "    \"\"\"\n",
    "\n",
    "    processor: AutoProcessor\n",
    "\n",
    "    def __call__(\n",
    "        self, features\n",
    "    ):\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        labels = [\n",
    "            torch.tensor(feature[\"labels\"]).transpose(0, 1) for feature in features\n",
    "        ]\n",
    "        # (bsz, seq_len, num_codebooks)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=-100\n",
    "        )\n",
    "\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        input_ids = self.processor.tokenizer.pad(input_ids, return_tensors=\"pt\")\n",
    "\n",
    "        batch = {\"labels\": labels, **input_ids}\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0acc8-b551-4008-a52e-239ce6534cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicgenTrainer(Seq2SeqTrainer):\n",
    "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
    "        if self.tokenizer is not None and hasattr(self.tokenizer, \"pad_token_id\"):\n",
    "            # If PAD token is not defined at least EOS token has to be defined\n",
    "            pad_token_id = (\n",
    "                self.tokenizer.pad_token_id\n",
    "                if self.tokenizer.pad_token_id is not None\n",
    "                else self.tokenizer.eos_token_id\n",
    "            )\n",
    "        else:\n",
    "            if self.model.config.pad_token_id is not None:\n",
    "                pad_token_id = self.model.config.pad_token_id\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Pad_token_id must be set in the configuration of the model, in order to pad tensors\"\n",
    "                )\n",
    "\n",
    "        padded_tensor = pad_token_id * torch.ones(\n",
    "            (tensor.shape[0], max_length, tensor.shape[2]),\n",
    "            dtype=tensor.dtype,\n",
    "            device=tensor.device,\n",
    "        )\n",
    "        length = min(max_length, tensor.shape[1])\n",
    "        padded_tensor[:, :length] = tensor[:, :length]\n",
    "        return padded_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
